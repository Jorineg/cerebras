param memcpy_params: comptime_struct;
param is_static_region: bool;
param rank: i16;
param steps: i16;
// param weights: [rank+1]f32;
param send_col: color; // Color used to send/recv data between PEs
param e_to_w_col: color; // Color used to send/recv data between PEs
param w_to_e_col: color; // Color used to send/recv data between PEs
param n_to_s_col: color; // Color used to send/recv data between PEs
param s_to_n_col: color; // Color used to send/recv data between PEs

var weights = [rank+1]f32 {1.0, 0.5};

// Queue IDs
const send_color_oq = @get_output_queue(1);
const recv_color_iq_w = @get_input_queue(2);
const recv_color_iq_e = @get_input_queue(3);
const recv_color_iq_n = @get_input_queue(4);
const recv_color_iq_s = @get_input_queue(5);

// Task ID used by a local task to unblock cmd stream
const exit_task_id: local_task_id = @get_local_task_id(9);
const send_task_id: local_task_id = @get_local_task_id(10);


// memcpy module provides infrastructure for copying data
// and launching functions from the host
const sys_mod = @import_module("<memcpy/memcpy>", memcpy_params);

var matrix: [1]f32 = [1]f32 {0.0};
var matrix_ptr: [*]f32 = &matrix;

// const x_src1_dsr = @get_dsr(dsr_src1, 5);

// DSDs for accessing weights and matrix
const weights_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{rank+1} -> weights[i] });
const matrix_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{1} -> matrix[0] });
const send_dsd = @get_dsd(fabout_dsd, .{ .fabric_color = send_col, .extent = 1, .output_queue = send_color_oq });

var n_dsds: [rank]fabin_dsd;
var s_dsds: [rank]fabin_dsd;
var w_dsds: [rank]fabin_dsd;
var e_dsds: [rank]fabin_dsd;

fn init_dsds() void {
  for (@range(i16, rank)) |i| {
    n_dsds[i] = @get_dsd(fabin_dsd, .{
                     .fabric_color = n_to_s_col, .extent = 1,
                     .input_queue = recv_color_iq_n
                   });
    s_dsds[i] = @get_dsd(fabin_dsd, .{
                     .fabric_color = s_to_n_col, .extent = 1,
                     .input_queue = recv_color_iq_s
                   });
    w_dsds[i] = @get_dsd(fabin_dsd, .{
                     .fabric_color = w_to_e_col, .extent = 1,
                     .input_queue = recv_color_iq_w
                   });
    e_dsds[i] = @get_dsd(fabin_dsd, .{
                     .fabric_color = e_to_w_col, .extent = 1,
                     .input_queue = recv_color_iq_e
                   });
  }
}


// function that initializes weights
// divide weights to not have to divide in the loop
// first weight is for center element, others are for neighbors
// so that sum of w[0] + 4*w[1] + 4*w[2] + ... + 4*w[rank] = 1
fn init_weights() void {
  var sum: f32 = 0.0;
  for (@range(i16, rank+1)) |i| {
    sum += if (i == 0) weights[i] else 4.0*weights[i];
  }
  for (@range(i16, rank+1)) |i| {
    weights[i] /= sum;
  }
}


// option 1: move everything to ram, then do the computation.
// Can use simd instructions and have same multiplier for different colors
// uses only 4 dsds

// option 2: have one dsd for every element to receive, perform computation while receiving
// there are enough (44 or 48) dsrs that coulb be assignend in advance

// option 3: can we "reset" a fabin dsd to use it again? 


// option 2:
fn step(exit: bool) void {
  // add own value to result

  // @fmacs(matrix_dsd, matrix_dsd, matrix_dsd, weights[0], .{ .async = true });
  @fmacs(matrix_dsd, matrix_dsd, matrix_dsd, weights[0]);

  // add values from neighbors
  for (@range(i16, rank)) |i| {
    // @fmacs(matrix_dsd, matrix_dsd, n_dsds[i], weights_dsd[i+1], .{ .async = true });
    // @fmacs(matrix_dsd, matrix_dsd, s_dsds[i], weights_dsd[i+1], .{ .async = true });
    // @fmacs(matrix_dsd, matrix_dsd, w_dsds[i], weights_dsd[i+1], .{ .async = true });
    // if (exit) {
    //   @fmacs(matrix_dsd, matrix_dsd, e_dsds[i], weights_dsd[i+1], .{ .async = true, .activate = exit_task_id });
    // }
    // else {
    //   @fmacs(matrix_dsd, matrix_dsd, e_dsds[i], weights_dsd[i+1], .{ .async = true, .activate = send_task_id });
    // }

    // synchroneous version
    @fmacs(matrix_dsd, matrix_dsd, n_dsds[i], weights[i+1]);
    @fmacs(matrix_dsd, matrix_dsd, s_dsds[i], weights[i+1]);
    @fmacs(matrix_dsd, matrix_dsd, w_dsds[i], weights[i+1]);
    @fmacs(matrix_dsd, matrix_dsd, e_dsds[i], weights[i+1]);
    if (exit) {
      @activate(exit_task_id);
    } else {
      @fmovs(send_dsd, matrix_dsd);
    }
  }
}

task send_to_neighbors() void {
  @fmovs(send_dsd, matrix_dsd, .{ .async = true });
}

// Call gemv function and send/ receive partial result y
fn compute() void {
  init_weights();
  init_dsds();
  @fmovs(send_dsd, matrix_dsd);
  @activate(exit_task_id);
  // for (@range(i16, steps)) |i| {
  //   step(i == steps-1);
  //   init_dsds(); // need to reinitialize dsds every iteration
  // }
}

task exit_task() void {
  sys_mod.unblock_cmd_stream();
}

comptime {
  // When exit_task_id is activated, exit_task will execute
  @bind_local_task(exit_task, exit_task_id);
  @bind_local_task(send_to_neighbors, send_task_id);

  // On WSE-3, we must explicitly initialize input and output queues
  if (@is_arch("wse3")) {
    @initialize_queue(send_color_oq, .{ .color = send_col });
    @initialize_queue(recv_color_iq_w, .{ .color = w_to_e_col });
    @initialize_queue(recv_color_iq_e, .{ .color = e_to_w_col });
    @initialize_queue(recv_color_iq_n, .{ .color = n_to_s_col });
    @initialize_queue(recv_color_iq_s, .{ .color = s_to_n_col });
  }
  @export_symbol(matrix_ptr, "matrix");
  @export_symbol(compute);
}